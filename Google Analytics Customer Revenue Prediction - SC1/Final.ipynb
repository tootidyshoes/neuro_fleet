{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color=\"blue\">Importing Libraries</font></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "import math\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVR \n",
    "from sklearn.metrics import mean_squared_error \n",
    "import xgboost as xgb\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color=\"red\"> Pipeline: </font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. To load DataFrame and convert json and make sub columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/julian3833/1-quick-start-read-csv-and-flatten-json-fields/notebook.\n",
    "json_columns = ['device', 'geoNetwork','totals', 'trafficSource']  #these 4 columns are of type jspn objects\n",
    "def load_dataframe(filename):\n",
    "    df = pd.read_csv(filename, converters={column: json.loads for column in json_columns}, \n",
    "                     dtype={'fullVisitorId': 'str'}) #loading json columns and specifying the type of fullvisitor id as string\n",
    "    \n",
    "    for column in json_columns:\n",
    "        column_as_df = json_normalize(df[column])  #normalizing json columns\n",
    "        column_as_df.columns = [f\"{column}_{subcolumn}\" for subcolumn in column_as_df.columns]  #creating subcolumns of json columns \n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)  #adding subcolumns and dropping the json columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Adding date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.python.org/3/library/datetime.html\n",
    "#adding datetime column in data\n",
    "def add_date_features(df):\n",
    "    df['date'] = df['date'].astype(str)\n",
    "    df[\"date\"] = df[\"date\"].apply(lambda x : x[:4] + \"-\" + x[4:6] + \"-\" + x[6:])\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    \n",
    "    df[\"month\"]   = df['date'].dt.month  #getting month\n",
    "    df[\"day\"]     = df['date'].dt.day   #getting day\n",
    "    df[\"weekday\"] = df['date'].dt.weekday   #getting date\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. To normalize the column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_numerical_columns(dataframe, isTrainDataset = True):\n",
    "    dataframe[\"totals_hits\"] = dataframe[\"totals_hits\"].astype(float)\n",
    "    #dataframe[\"totals.hits\"] = (dataframe[\"totals.hits\"] - min(dataframe[\"totals.hits\"])) / (max(dataframe[\"totals.hits\"]) - min(dataframe[\"totals.hits\"]))\n",
    "\n",
    "    dataframe[\"totals_pageviews\"] = dataframe[\"totals_pageviews\"].astype(float)\n",
    "    #dataframe[\"totals.pageviews\"] = (dataframe[\"totals.pageviews\"] - min(dataframe[\"totals.pageviews\"])) / (max(dataframe[\"totals.pageviews\"]) - min(dataframe[\"totals.pageviews\"]))\n",
    "    \n",
    "    dataframe[\"totals_bounces\"] = dataframe[\"totals_bounces\"].astype(float)\n",
    "    dataframe[\"totals_newVisits\"] = dataframe[\"totals_newVisits\"].astype(float)\n",
    "    \n",
    "    \n",
    "    if isTrainDataset:\n",
    "        dataframe[\"totals_transactionRevenue\"] = dataframe[\"totals_transactionRevenue\"].fillna(0.0)\n",
    "    return dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You just need to paas the raw data (.csv file) in th get prediction class and prediction results will get saved in your local disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Final_fun_1 will take .csv file as input and return the predictions, and saves the predicted values and its respected id in .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_fun_1(file,model,save_in):\n",
    "    filename = file\n",
    "    \n",
    "    \n",
    "    test_data = load_dataframe(filename)\n",
    "    \n",
    "    test_data = add_date_features(test_data)\n",
    "    \n",
    "    test_data[\"totals_hits\"] = test_data[\"totals_hits\"].astype(float)\n",
    "    test_data[\"totals_pageviews\"] = test_data[\"totals_pageviews\"].astype(float)\n",
    "    test_data[\"totals_bounces\"] = test_data[\"totals_bounces\"].astype(float)\n",
    "    test_data[\"totals_newVisits\"] = test_data[\"totals_newVisits\"].astype(float)\n",
    "    \n",
    "    \n",
    "    \n",
    "    constant_columns = [column for column in test_data.columns if test_data[column].nunique(dropna=False)==1]\n",
    "\n",
    "    #dropping constant columns\n",
    "    test_data = test_data.drop(columns=constant_columns)\n",
    "\n",
    "    #Sorting by date to perform time based slicing\n",
    "    test_data = test_data.sort_values(by='date',ascending=True)\n",
    "    \n",
    "    submission = pd.DataFrame()\n",
    "    submission[\"fullVisitorId\"] = test_data[\"fullVisitorId\"]\n",
    "    submission['predictedLogRevenue'] = np.nan\n",
    "    ## non relevant columns\n",
    "    non_relevant = [\"visitNumber\", \"date\", \"fullVisitorId\", \"sessionId\", \"visitId\", \"visitStartTime\"]\n",
    "\n",
    "    ## Droping non relevant columns\n",
    "    test = test_data.drop(columns=non_relevant)\n",
    "\n",
    "    categorical_features_test = (test.select_dtypes(include=[np.object]))\n",
    "    categorical_columns = [column for column in test.columns if not column.startswith('total')]\n",
    "    categorical_columns = [column for column in categorical_features_test if column not in constant_columns + non_relevant]\n",
    "\n",
    "    for column in categorical_columns:\n",
    "\n",
    "        le = LabelEncoder()\n",
    "\n",
    "        test_values = list(test[column].values.astype(str))\n",
    "\n",
    "        le.fit(test_values)\n",
    "\n",
    "        test[column] = le.transform(test_values) \n",
    "\n",
    "    test['device_isMobile'] = le.transform(test_values)\n",
    "\n",
    "\n",
    "\n",
    "    test = normalize_numerical_columns(test,isTrainDataset=False)\n",
    "\n",
    "    test = test.fillna(0).astype('float32')\n",
    "\n",
    "    test['mean_pageViews_per_networkDomain'] = test.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('mean').astype('int')\n",
    "    test['mean_hits_per_networkDomain'] = test.groupby('geoNetwork_networkDomain')['totals_hits'].transform('mean').astype('int')\n",
    "\n",
    "    test = test.drop('channelGrouping',axis=1)\n",
    "    \n",
    "    \n",
    "    # load the model from disk\n",
    "    loaded_model = pkl.load(open(model, 'rb'))\n",
    "    \n",
    "    test.columns = loaded_model.feature_names_\n",
    "    submission['predictedLogRevenue'] = np.log(loaded_model.predict(test.values))\n",
    "    submission.to_csv(save_in)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:24:18] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vansh\\Anaconda3\\envs\\nenv\\lib\\site-packages\\ipykernel_launcher.py:65: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in log\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_fun_1('test.csv','final_model.pkl','predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. final_fun_2 will take .csv file as input and make prediction, compares the predicted result with ground truth and given \"root mean square error\" and plots feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_fun_2(file,model):\n",
    "    filename = file\n",
    "    \n",
    "        \n",
    "    test_data = load_dataframe(filename)\n",
    "    \n",
    "    test_data = add_date_features(test_data)\n",
    "    \n",
    "    test_data[\"totals_hits\"] = test_data[\"totals_hits\"].astype(float)\n",
    "    test_data[\"totals_pageviews\"] = test_data[\"totals_pageviews\"].astype(float)\n",
    "    test_data[\"totals_bounces\"] = test_data[\"totals_bounces\"].astype(float)\n",
    "    test_data[\"totals_newVisits\"] = test_data[\"totals_newVisits\"].astype(float)\n",
    "    \n",
    "    \n",
    "    \n",
    "    constant_columns = [column for column in test_data.columns if test_data[column].nunique(dropna=False)==1]\n",
    "\n",
    "    #dropping constant columns\n",
    "    test_data = test_data.drop(columns=constant_columns)\n",
    "\n",
    "    #Sorting by date to perform time based slicing\n",
    "    test_data = test_data.sort_values(by='date',ascending=True)\n",
    "    \n",
    "   \n",
    "    ## non relevant columns\n",
    "    non_relevant = [\"visitNumber\", \"date\", \"fullVisitorId\", \"sessionId\", \"visitId\", \"visitStartTime\"]\n",
    "\n",
    "    ## Droping non relevant columns\n",
    "    test = test_data.drop(columns=non_relevant)\n",
    "\n",
    "    categorical_features_test = (test.select_dtypes(include=[np.object]))\n",
    "    categorical_columns = [column for column in test.columns if not column.startswith('total')]\n",
    "    categorical_columns = [column for column in categorical_features_test if column not in constant_columns + non_relevant]\n",
    "\n",
    "    for column in categorical_columns:\n",
    "\n",
    "        le = LabelEncoder()\n",
    "\n",
    "        test_values = list(test[column].values.astype(str))\n",
    "\n",
    "        le.fit(test_values)\n",
    "\n",
    "        test[column] = le.transform(test_values) \n",
    "\n",
    "    test['device_isMobile'] = le.transform(test_values)\n",
    "\n",
    "\n",
    "\n",
    "    test = normalize_numerical_columns(test,isTrainDataset=False)\n",
    "\n",
    "    test = test.fillna(0).astype('float32')\n",
    "\n",
    "    test['mean_pageViews_per_networkDomain'] = test.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('mean').astype('int')\n",
    "    test['mean_hits_per_networkDomain'] = test.groupby('geoNetwork_networkDomain')['totals_hits'].transform('mean').astype('int')\n",
    "\n",
    "    test = test.drop('channelGrouping',axis=1)\n",
    "    \n",
    "    \n",
    "    # load the model from disk\n",
    "    loaded_model = pkl.load(open(model, 'rb'))\n",
    "    \n",
    "    test.columns = loaded_model.feature_names_\n",
    "    \n",
    "    predictions = np.log(loaded_model.predict(test.values))\n",
    "    \n",
    "    mse =mean_squared_error(truth_value, predictions)\n",
    "    rmse = math.sqrt(mse)\n",
    "    print(\"Root Mean Squared Error:\", rmse)\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "   \n",
    "    importances = loaded_model.feature_importances_\n",
    "    \n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Rearrange feature names so they match the sorted feature importances\n",
    "    names = [test.columns[i] for i in indices]\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure()\n",
    "\n",
    "    # Create plot title\n",
    "    plt.title(\"Feature Importance\")\n",
    "\n",
    "    # Add bars\n",
    "    plt.bar(range(30), importances[indices])\n",
    "\n",
    "    # Add feature names as x-axis labels\n",
    "    plt.xticks(range(30),names, rotation=90)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fun_2('test.csv','final_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Since in test.csv we are not given \"ground truth\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
